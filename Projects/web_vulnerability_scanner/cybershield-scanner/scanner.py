import argparse
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse, urlencode, parse_qs
import os
import json
from datetime import datetime
from jinja2 import Environment, FileSystemLoader
import sys # <--- เพิ่ม import นี้

# --- Wordlists และ Payloads ---
XSS_PAYLOADS = [
    "<script>alert('XSS')</script>",
    "'';!--\"<XSS>=&{()}",
    "<img src=x onerror=alert('XSS')>"
]

SQLI_PAYLOADS = ["'", "\"", "' OR 1=1 -- ", '" OR 1=1 -- ']

SQLI_ERROR_KEYWORDS = [
    "sql syntax error", "mysql_fetch_array()", "You have an error in your SQL syntax",
    "Warning: mysql_fetch_assoc()", "sql command not properly ended",
    "Microsoft OLE DB Provider for ODBC Drivers error"
]

LFI_PAYLOADS = [
    "../../../../../../etc/passwd",
    "../../../../../../Windows/win.ini",
    "../../../../../../proc/self/cmdline"
]

# --- Helper Functions ---
def get_forms(url):
    """ดึงข้อมูล forms ทั้งหมดจาก URL"""
    try:
        response = requests.get(url, timeout=10)
        soup = BeautifulSoup(response.content, "html.parser")
        return soup.find_all("form")
    except requests.exceptions.RequestException as e:
        print(f"[-] Error getting forms from {url}: {e}")
        return []

def get_links_with_params(url):
    """ดึง URL ที่มีพารามิเตอร์จากหน้าเว็บ"""
    links = set()
    base_url = urlparse(url)
    try:
        response = requests.get(url, timeout=10)
        soup = BeautifulSoup(response.content, "html.parser")
        for a_tag in soup.find_all("a", href=True):
            full_url = urljoin(url, a_tag['href'])
            if urlparse(full_url).netloc == base_url.netloc and '?' in full_url:
                links.add(full_url)
    except requests.exceptions.RequestException as e:
        print(f"[-] Error getting links with parameters: {e}")
    return list(links)

def web_crawler(url):
    """รวบรวม URL ที่เกี่ยวข้องจากเว็บไซต์"""
    urls_to_scan = {url}
    base_url = urlparse(url).netloc
    
    print(f"[+] Starting web crawler on {url}...")
    
    try:
        response = requests.get(url, timeout=10)
        soup = BeautifulSoup(response.content, "html.parser")
        
        for link in soup.find_all("a", href=True):
            full_url = urljoin(url, link.get("href"))
            if urlparse(full_url).netloc == base_url:
                urls_to_scan.add(full_url)
                
    except requests.exceptions.RequestException as e:
        print(f"[-] Error crawling URL: {e}")

    print(f"[+] Found {len(urls_to_scan)} URLs to scan.")
    return list(urls_to_scan)

# --- Scanning Functions ---
def scan_sql_injection(url):
    """สแกนหาช่องโหว่ SQL Injection ขั้นพื้นฐาน"""
    vulnerabilities = []
    print("[+] Scanning for SQL Injection...")
    
    links_to_test = [url] if '?' in url else get_links_with_params(url)
    
    for test_url in links_to_test:
        print(f"[*] Testing: {test_url}")
        parsed_url = urlparse(test_url)
        params = parse_qs(parsed_url.query)
        
        for param_name, values in params.items():
            for payload in SQLI_PAYLOADS:
                test_params = params.copy()
                test_params[param_name] = [f"{values[0]}{payload}"]
                
                encoded_params = urlencode(test_params, doseq=True)
                full_test_url = parsed_url._replace(query=encoded_params).geturl()
                
                try:
                    response = requests.get(full_test_url, timeout=5)
                    for keyword in SQLI_ERROR_KEYWORDS:
                        if keyword in response.text.lower():
                            vulnerability = {
                                "type": "SQL Injection", 
                                "description": f"Payload '{payload}' triggered SQL error",
                                "url": full_test_url
                            }
                            print(f"[!] SQL Injection vulnerability found at {full_test_url} with payload: {payload}")
                            vulnerabilities.append(vulnerability)
                            break
                except requests.exceptions.RequestException as e:
                    print(f"[-] Error during SQL Injection scan on {full_test_url}: {e}")
    
    if not vulnerabilities:
        print("[+] SQL Injection scan finished. No vulnerabilities found.")
    
    return vulnerabilities

def scan_xss(url):
    """สแกนหาช่องโหว่ Cross-Site Scripting (XSS)"""
    vulnerabilities = []
    print("[+] Scanning for XSS...")
    
    forms = get_forms(url)
    for form in forms:
        action = form.get("action")
        post_url = urljoin(url, action)
        
        for payload in XSS_PAYLOADS:
            data = {}
            inputs = form.find_all("input")
            for input_tag in inputs:
                input_name = input_tag.get("name")
                input_type = input_tag.get("type", "text")
                if input_type == "text":
                    data[input_name] = payload
            
            try:
                response = requests.post(post_url, data=data, timeout=5)
                if payload in response.text:
                    vulnerability = {
                        "type": "XSS (Form)", 
                        "description": f"Payload '{payload}' reflected in form input",
                        "url": post_url
                    }
                    print(f"[!] XSS Vulnerability found in form at {post_url} with payload: {payload}")
                    vulnerabilities.append(vulnerability)
            except requests.exceptions.RequestException:
                pass
    
    links_with_params = [url] if '?' in url else get_links_with_params(url)
    for test_url in links_with_params:
        parsed_url = urlparse(test_url)
        params = parse_qs(parsed_url.query)
        
        for param_name, values in params.items():
            for payload in XSS_PAYLOADS:
                test_params = params.copy()
                test_params[param_name] = [payload]
                encoded_params = urlencode(test_params, doseq=True)
                full_test_url = parsed_url._replace(query=encoded_params).geturl()
                
                try:
                    response = requests.get(full_test_url, timeout=5)
                    if payload in response.text:
                        vulnerability = {
                            "type": "XSS (URL)", 
                            "description": f"Payload '{payload}' reflected in URL parameter '{param_name}'",
                            "url": full_test_url
                        }
                        print(f"[!] XSS Vulnerability found in URL at {full_test_url} with payload: {payload}")
                        vulnerabilities.append(vulnerability)
                except requests.exceptions.RequestException:
                    pass

    if not vulnerabilities:
        print("[+] XSS scan finished. No vulnerabilities found.")
    
    return vulnerabilities

def directory_bruteforce(url, wordlist_path="wordlists/directories.txt"):
    """สแกนหา directories และไฟล์ที่ไม่ได้ป้องกัน"""
    vulnerabilities = []
    print(f"[+] Starting directory brute-force scan on {url}...")
    try:
        if not os.path.exists(wordlist_path):
            print(f"[-] Wordlist file not found at {wordlist_path}. Skipping this scan.")
            return vulnerabilities

        with open(wordlist_path, "r") as f:
            for line in f:
                directory = line.strip()
                if not directory: continue
                target_url = url.rstrip('/') + "/" + directory
                try:
                    response = requests.get(target_url, timeout=5)
                    if response.status_code == 200:
                        vulnerability = {
                            "type": "Directory Traversal", 
                            "description": f"Directory '{directory}' found with 200 OK status.",
                            "url": target_url
                        }
                        print(f"[!] Found: {target_url} (Status: 200 OK)")
                        vulnerabilities.append(vulnerability)
                except requests.exceptions.RequestException:
                    pass
    except Exception as e:
        print(f"[-] Error during Directory Brute-force: {e}")
    
    if not vulnerabilities:
        print("[+] Directory brute-force finished. No vulnerabilities found.")
    
    return vulnerabilities

def scan_lfi(url):
    """สแกนหาช่องโหว่ Local File Inclusion (LFI)"""
    vulnerabilities = []
    print("[+] Scanning for LFI...")
    
    if '?' not in url:
        print("[-] URL does not have parameters. Skipping LFI scan.")
        return []
    
    parsed_url = urlparse(url)
    params = parse_qs(parsed_url.query)
    
    for param_name, values in params.items():
        for payload in LFI_PAYLOADS:
            test_params = params.copy()
            test_params[param_name] = [payload]
            encoded_params = urlencode(test_params, doseq=True)
            full_test_url = parsed_url._replace(query=encoded_params).geturl()
            
            try:
                response = requests.get(full_test_url, timeout=5)
                if "root:" in response.text or "for 16-bit app support" in response.text:
                    vulnerability = {
                        "type": "LFI", 
                        "description": f"LFI vulnerability found with payload: '{payload}'",
                        "url": full_test_url
                    }
                    print(f"[!] LFI vulnerability found at {full_test_url} with payload: {payload}")
                    vulnerabilities.append(vulnerability)
                    break
            except requests.exceptions.RequestException:
                pass
            
    if not vulnerabilities:
        print("[+] LFI scan finished. No vulnerabilities found.")
    
    return vulnerabilities

# --- Reporting Function ---
def generate_report(url, vulnerabilities):
    """สร้างรายงานการสแกนเป็น HTML string"""
    report = {
        "target_url": url,
        "vulnerabilities_found": vulnerabilities,
        "scan_time": str(datetime.now())
    }
    
    env = Environment(loader=FileSystemLoader('.'))
    template = env.get_template('report_template.html')
    html_report_content = template.render(report=report)
    
    return html_report_content

# --- Main CLI Logic ---
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="A simple Web Vulnerability Scanner.")
    parser.add_argument("--url", help="Target URL to scan (e.g., https://example.com)", required=True)
    parser.add_argument("--sqli", action="store_true", help="Enable SQL Injection scan.")
    parser.add_argument("--xss", action="store_true", help="Enable XSS scan.")
    parser.add_argument("--dir", action="store_true", help="Enable Directory Brute-force.")
    parser.add_argument("--lfi", action="store_true", help="Enable Local File Inclusion scan.")
    parser.add_argument("--crawler", action="store_true", help="Enable web crawling to find more URLs to scan.")
    parser.add_argument("--all", action="store_true", help="Enable all scans.")
    
    args = parser.parse_args()
    target_url = args.url
    vulnerabilities = []
    
    # ... (ส่วนการรันสแกนเหมือนเดิม) ...
    
    run_all = args.all or (not args.sqli and not args.xss and not args.dir and not args.lfi)
    
    urls_to_scan = [target_url]
    if args.crawler:
        urls_to_scan = web_crawler(target_url)

    for url_to_scan in urls_to_scan:
        print(f"\nScanning URL: {url_to_scan}")

        if run_all or args.sqli:
            found_sqli = scan_sql_injection(url_to_scan)
            vulnerabilities.extend(found_sqli)
            print("-" * 50)
        
        if run_all or args.xss:
            found_xss = scan_xss(url_to_scan)
            vulnerabilities.extend(found_xss)
            print("-" * 50)
        
        if run_all or args.dir:
            found_dir = directory_bruteforce(url_to_scan)
            vulnerabilities.extend(found_dir)
            print("-" * 50)
        
        if run_all or args.lfi:
            found_lfi = scan_lfi(url_to_scan)
            vulnerabilities.extend(found_lfi)
            print("-" * 50)
    
    html_report = generate_report(target_url, vulnerabilities)
    
    # ส่วนที่ถูกแก้ไข: พิมพ์ผลลัพธ์ออกทางหน้าจอด้วยการเข้ารหัส utf-8
    sys.stdout.buffer.write(html_report.encode('utf-8'))